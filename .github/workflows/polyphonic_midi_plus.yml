name: Advanced Polyphonic MIDI Transcription

on:
  workflow_dispatch:
  push:
    paths:
      - "**/*.mp3"
      - "**/*.wav"

jobs:
  polyphonic_plus:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.10"

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg

    - name: Install Python packages
      run: |
        python -m pip install --upgrade pip
        pip install numpy scipy librosa soundfile pretty_midi spleeter matplotlib

    - name: Find audio files
      run: |
        find . -type f \( -iname "*.mp3" -o -iname "*.wav" \) > audio_files.txt
        cat audio_files.txt

    - name: Convert MP3 → WAV
      run: |
        mkdir -p wav_inputs
        while IFS= read -r file; do
          if [[ "$file" == *.mp3 ]]; then
            base=$(basename "$file" .mp3)
            ffmpeg -y -i "$file" -ar 16000 -ac 1 "wav_inputs/$base.wav"
          else
            cp "$file" wav_inputs/
          fi
        done < audio_files.txt

    - name: Add upgraded polyphonic transcriber
      run: |
        mkdir -p tools
        cat > tools/polyphonic_plus.py <<'PY'
#!/usr/bin/env python3
"""
Advanced offline polyphonic transcription
Features:
  - 4-stem Spleeter separation
  - Harmonic salience multi-pitch detection
  - Per-stem multi-track MIDI
  - Automatic tempo detection & quantization
  - Piano roll PNG export
"""
import os, math, glob
import numpy as np
import librosa, librosa.display
import pretty_midi
import soundfile as sf
import matplotlib.pyplot as plt

SAMPLE_RATE = 16000
HOP = 512
NFFT = 2048
TOP_N = 8
MIN_NOTE_DURATION = 0.05
MIN_GAP = 0.03
STEMS = ["vocals", "drums", "bass", "other"]
PROGRAMS = {"vocals": 52, "drums": 0, "bass": 33, "other": 24}

# Try loading Spleeter (4-stem)
try:
    from spleeter.separator import Separator
    SEP = Separator("spleeter:4stems")
except Exception as e:
    SEP = None
    print("⚠️ Spleeter not available, running without separation:", e)

def separate_stems(wav):
    if SEP is None: return {"mix": wav}
    base = os.path.splitext(os.path.basename(wav))[0]
    out_dir = f"spleeter_out/{base}"
    try:
        SEP.separate_to_file(wav, "spleeter_out", codec='wav', synchronous=True)
        stems = {}
        for stem in STEMS:
            path = os.path.join(out_dir, f"{stem}.wav")
            if os.path.exists(path):
                stems[stem] = path
        return stems if stems else {"mix": wav}
    except Exception as e:
        print("Separation failed:", e)
        return {"mix": wav}

def harmonic_salience(y, sr):
    S = np.abs(librosa.stft(y, n_fft=NFFT, hop_length=HOP))
    freqs = librosa.fft_frequencies(sr=sr, n_fft=NFFT)
    w = 1 + 0.5 * np.cos(np.linspace(-np.pi, np.pi, len(freqs)))
    S *= w[:, None]
    mag_db = librosa.amplitude_to_db(S, ref=np.max)
    times = librosa.frames_to_time(np.arange(S.shape[1]), sr=sr, hop_length=HOP)
    frames = []
    for i in range(S.shape[1]):
        mag_col = S[:, i]
        threshold = np.max(mag_col) * 0.12
        idx = np.where(mag_col > threshold)[0]
        candidates = []
        for ix in idx:
            if mag_db[ix, i] < -70: continue
            f = freqs[ix]
            if 30 < f < 6000:
                candidates.append(f)
        if len(candidates) > TOP_N:
            bins = {f: mag_col[np.argmin(abs(freqs-f))] for f in candidates}
            candidates = sorted(candidates, key=lambda f: bins[f], reverse=True)[:TOP_N]
        frames.append(sorted(set(candidates)))
    return times, frames

def freq_to_midi(f):
    return None if f <= 0 else int(round(69 + 12 * math.log2(f/440.0)))

def frame_velocity(frame):
    rms = np.sqrt(np.mean(frame * frame)) + 1e-12
    vel = int(np.clip(30 + 100 * rms / 0.1, 20, 127))
    return vel

def build_notes(times, frames, y, sr):
    active, notes = {}, []
    frame_len = int(sr * (times[1]-times[0])) if len(times)>1 else 512
    for i, cand in enumerate(frames):
        midi_set = [freq_to_midi(f) for f in cand if f is not None]
        start = i*HOP
        frame = y[start:start+frame_len]
        vel = frame_velocity(frame)
        # end finished notes
        to_end = [m for m in active if m not in midi_set]
        for m in to_end:
            s0, l0, vsum, n = active.pop(m)
            s, e = times[s0], times[l0]+(times[1]-times[0])
            if e-s>=MIN_NOTE_DURATION:
                notes.append((m,s,e,int(vsum/n)))
        # extend new
        for m in midi_set:
            if m in active:
                s0, _, vsum, n = active[m]
                active[m]=(s0,i,vsum+vel,n+1)
            else:
                active[m]=(i,i,vel,1)
    for m,(s0,l0,vsum,n) in active.items():
        s,e=times[s0],times[l0]+(times[1]-times[0])
        if e-s>=MIN_NOTE_DURATION:
            notes.append((m,s,e,int(vsum/n)))
    notes.sort(key=lambda x:x[1])
    return merge_notes(notes)

def merge_notes(notes):
    merged=[]
    by_pitch={}
    for m,s,e,v in notes:
        by_pitch.setdefault(m,[]).append((s,e,v))
    for m,segs in by_pitch.items():
        segs.sort()
        cs,ce,cv=segs[0];count=1
        for s,e,v in segs[1:]:
            if s-ce<=MIN_GAP:
                ce=max(ce,e);cv=(cv*count+v)/(count+1);count+=1
            else:
                merged.append((m,cs,ce,int(cv)))
                cs,ce,cv=s,e,v;count=1
        merged.append((m,cs,ce,int(cv)))
    merged.sort(key=lambda x:x[1])
    return merged

def detect_tempo(y, sr):
    tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
    return tempo, beats

def quantize_notes(notes, tempo, beats, sr):
    if len(beats)<2: return notes
    beat_times = librosa.frames_to_time(beats, sr=sr)
    quantized=[]
    grid = np.linspace(beat_times[0], beat_times[-1], len(beat_times)*4)
    def snap(t): return grid[np.argmin(np.abs(grid-t))]
    for m,s,e,v in notes:
        qs,qe=snap(s),snap(e)
        if qe-qs>0.03:
            quantized.append((m,qs,qe,v))
    return quantized

def save_midi(stem_notes, out_path):
    pm = pretty_midi.PrettyMIDI()
    for stem, notes in stem_notes.items():
        inst = pretty_midi.Instrument(program=PROGRAMS.get(stem,0), name=stem)
        for m,s,e,v in notes:
            inst.notes.append(pretty_midi.Note(velocity=v,pitch=m,start=s,end=e))
        pm.instruments.append(inst)
    pm.write(out_path)

def piano_roll_image(pm, out_png):
    fig, ax = plt.subplots(figsize=(12,4))
    for inst in pm.instruments:
        color = np.random.rand(3,)
        for note in inst.notes:
            ax.plot([note.start, note.end],[note.pitch,note.pitch],lw=4,color=color)
    ax.set_xlabel("Time (s)")
    ax.set_ylabel("MIDI Pitch")
    ax.set_title("Piano Roll")
    plt.tight_layout()
    fig.savefig(out_png)
    plt.close(fig)

def transcribe(wav):
    stems = separate_stems(wav)
    stem_notes={}
    for stem, path in stems.items():
        y, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)
        tempo, beats = detect_tempo(y, sr)
        times, frames = harmonic_salience(y, sr)
        notes = build_notes(times, frames, y, sr)
        qnotes = quantize_notes(notes, tempo, beats, sr)
        stem_notes[stem]=qnotes
    base=os.path.splitext(os.path.basename(wav))[0]
    os.makedirs("midi_output",exist_ok=True)
    mid_out=f"midi_output/{base}_poly_multi.mid"
    save_midi(stem_notes, mid_out)
    piano_roll_image(pretty_midi.PrettyMIDI(mid_out), mid_out.replace(".mid",".png"))
    print("✅ Saved:", mid_out)

def main():
    for wav in sorted(glob.glob("wav_inputs/*.wav")):
        transcribe(wav)

if __name__=="__main__":
    main()
PY

    - name: Run Advanced Transcriber
      run: python tools/polyphonic_plus.py

    - name: Upload MIDI and PNG outputs
      uses: actions/upload-artifact@v4
      with:
        name: midi_output
        path: midi_output/
